{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Report:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vivian Tan (vyt73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall project is based on applications of language models, more specifically text generation based on n-grams. Random text generation can be useful for designers to use in different settings such as web applications, site templates, and topography demos.  \n",
    "\n",
    "I will be using the n-gram language model to complete this project, starting with bigrams and then moving to trigrams if time permits. I will also use Maximum Likelihood Estimation (MLE) to calculate the probabilities used to select subsequent words. To store the texts, I will be using lists of strings/tokens for each work. Eventually, I hope to combind different works from the same author into a single list. \n",
    "\n",
    "I'll use texts from Project Gutenberg as my corpus, more specifically, from these Authors: (may remove/add authors based on progress)\n",
    "\n",
    "- Jane Austen\n",
    "- G.K. Chesterton\n",
    "- William Shakespeare\n",
    "- Arthur Conan Doyle\n",
    "- Fyodor Dostoyevsky\n",
    "\n",
    "Below I will show and discuss my initial results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initial results: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on applications of language models, more specifically text generation based on n-grams. \n",
    "The end goal is to generate a random text based on two different author's writing styles, but for this initial report we will: \n",
    "\n",
    "- explore different methods to generate a random text from a single work \n",
    "- explore different techniques to get works from URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate a random text from a single work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports: \n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using bigrams and MLE by hand: (based on hw code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first technique is from one of the homework assignments. It generates text by counting the bigrams for a text and calculating the probabilities for each then using these to generate the next word. It could easily be modified to account for trigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_and_prob(corpus, count, prob):\n",
    "\tcount2 = {}\n",
    "\t# Count bigrams and instantiate probability for each word to 0\n",
    "\tfor word1, word2 in nltk.bigrams(corpus):\n",
    "\t    if word1 not in count:\n",
    "\t        count[word1] = { }\n",
    "\t        count2[word1] = 0\n",
    "\t        prob[word1] = {}\n",
    "\n",
    "\t    if word2 not in count[word1]:\n",
    "\t    \t#print(word1, word2)\n",
    "\t    \tcount[word1][word2] = 1\n",
    "\t    \tcount2[word1] += 1\n",
    "\t    else:\n",
    "\t        count[word1][word2] += 1\n",
    "\t        count2[word1] += 1\n",
    "\n",
    "\t# Recall: To estimate probabilities of bigrams: \n",
    "\t# P(word2|word1) = count(word1, word2)/count(word1, ...)\n",
    "\n",
    "\t# for each word, calculate probability of bigram(s)\n",
    "\tfor word1 in count:\n",
    "\t\tfor word2 in count[word1]:\n",
    "\t\t\t# print(word1, word2,':', count[word1][word2]/count2[word1])\n",
    "\t\t\tprob[word1][word2] = count[word1][word2]/count2[word1]\n",
    "\n",
    "\treturn count, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the next word based on probabilities\n",
    "def gen_next_word(word, prob_dict):\n",
    "\tboundary = []\n",
    "\twords = []\n",
    "\tcount = 0\n",
    "\n",
    "\t# Generate random number\n",
    "\tdart = random.random()\n",
    "\n",
    "\tif(len(prob_dict) == 0): return 'ERROR'\n",
    "\n",
    "\t# Get probabilities and words\n",
    "\tfor w in prob_dict[word]:\n",
    "\t\t# get first probability and first word\n",
    "\t\tboundary.append(prob_dict[word][w])\n",
    "\t\twords.append(w)\n",
    "\t\tcount += 1\n",
    "\n",
    "\t#print('boundary:',boundary)\n",
    "\t#print('words:', words)\n",
    "\n",
    "\t# return the next word based on the dart value and boundaries\n",
    "\t# if there is only one option, return the word:\n",
    "\tif count == 1:\n",
    "\t\treturn words[0]\n",
    "\telse:\n",
    "\t\t#if there are more options, find the minimum\n",
    "\t\t#sort lists together \n",
    "\t\tboundary, words = (list(t) for t in zip(*sorted(zip(boundary, words))))\n",
    "\n",
    "\t\t# go through boundaries, starting with smallest \n",
    "\t\t# (Note: though this assignment could be completed w/a simple if else, \n",
    "\t\t# I wanted to try and write code that could work with trigrams and up)\n",
    "\t\ttotal = 0;\n",
    "\t\tfor i in range(0,len(boundary)):\n",
    "\t\t\tif i==0:\n",
    "\t\t\t\ttotal = boundary[i]\n",
    "\t\t\t\tif dart<=total:\n",
    "\t\t\t\t\treturn words[i]\n",
    "\t\t\t# add previous probabilities \n",
    "\t\t\telif dart<=(boundary[i]+total):\n",
    "\t\t\t\t\treturn words[i]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttotal += boundary[i]\n",
    "\t\t\t\t# print('look again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the text generation with \"Emma\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "emma[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating random text: Emma\n",
      "\n",
      "I will laugh . Her silence . Wingfield considers the gentleman ' s accommodation and Mr . Either bathing places in , her , I have been a repetition of England -- to encroach .-- A situation -- and felt -- unaccountable as what letter from her through such feelings "
     ]
    }
   ],
   "source": [
    "start = 'I'\n",
    "\n",
    "count_dict = {}\n",
    "prob_dict = {}\n",
    "count_dict, prob_dict = count_and_prob(emma, count_dict, prob_dict)\n",
    "next_word = start\n",
    "print('\\n\\nGenerating random text: Emma\\n')\n",
    "for i in range(0,50):\n",
    "\tprint(next_word, end =' ')\n",
    "\tnext_word = gen_next_word(next_word, prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that we can generate a fairly coherent paragraph based on a single text using this technique. Let's try another technique:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text using nltk: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: http://www.katrinerk.com/courses/python-worksheets/language-models-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique requires less lines of code since it just uses some built-in nltk functions. However, this method is limited to only bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a system of the amiable and the deepest humiliation to sensations had passed . They all your answer for her remains of the subject , in feature works .\" Emma ) that the _first_ with us with so properly struck and you should he had been proposing to "
     ]
    }
   ],
   "source": [
    "cfreq_emma_2gram = nltk.ConditionalFreqDist(nltk.bigrams(emma))\n",
    "cprob_emma_2gram = nltk.ConditionalProbDist(cfreq_emma_2gram, nltk.MLEProbDist)\n",
    "word = 'I'\n",
    "for i in range(0,50):\n",
    "    print(word, end=' ')\n",
    "    word = cprob_emma_2gram[word].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique can also generate a paragraph based on a single text, but it doesn't quite seem as cohesive as the previous generated paragraph. It's sufficient for our purposes though. I think I may use the 2nd technique if I want to test text generation quickly and use the 1st technique with trigrams for the final result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that we can generate text from a single work, the next step would be to generate a single text from multiple works. To do that, we need to know how to pull multiple works from online sources. Next we will explore different techniques to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting multiple works online:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through the nltk corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk gutenberg corpus comes with a few works, let's see what they have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "t1 = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "t2 = nltk.corpus.gutenberg.words('austen-persuasion.txt')\n",
    "t3 = nltk.corpus.gutenberg.words('austen-sense.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this corpus doesn't have books from all the authors we want, or the complete works of the authors we do have. However, if we decide to scale our project down to these works, we could save each of the texts and then concatenate them later. Let's see if we can get books directly from the Gutenberg website: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Through URLS: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Austen books: \n",
    "\n",
    "##### Emma\n",
    "\"https://www.gutenberg.org/files/158/158-0.txt\"\n",
    "\n",
    "##### Pride and Prejudice\n",
    "\"http://www.gutenberg.org/cache/epub/42671/pg42671.txt\"\n",
    "\n",
    "##### Love and Friendship\n",
    "\"https://www.gutenberg.org/files/1212/1212-0.txt\"\n",
    "\n",
    "##### Mansfield Park\n",
    "\"https://www.gutenberg.org/files/141/141-0.txt\"\n",
    "\n",
    "##### Northanger Abbey\n",
    "\"https://www.gutenberg.org/files/121/121-0.txt\"\n",
    "\n",
    "##### Persuasion\n",
    "\"https://www.gutenberg.org/files/121/121-0.txt\"\n",
    "\n",
    "##### Sense and Sensibility \n",
    "\"http://www.gutenberg.org/cache/epub/161/pg161.txt\"\n",
    "\n",
    "##### Lady Susan\n",
    "\"http://www.gutenberg.org/cache/epub/946/pg946.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook, Pride and Prejudice, by Jane Austen, Edited\\r\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "# Austen: \n",
    "# Pride and Prejudice\n",
    "url = \"http://www.gutenberg.org/cache/epub/42671/pg42671.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a text file using the url successfully! Let's see if we can quickly generate a text with it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do all . It was no more than once . '' said Jane as her , the summer . '' she was forced to be attempted to meet , a servant , on my happiness , `` Not that best politeness on any useful acquaintance entirely deceived herself . "
     ]
    }
   ],
   "source": [
    "cfreq_pride_2gram = nltk.ConditionalFreqDist(nltk.bigrams(tokens))\n",
    "cprob_pride_2gram = nltk.ConditionalProbDist(cfreq_pride_2gram, nltk.MLEProbDist)\n",
    "word = 'I'\n",
    "for i in range(0,50):\n",
    "    print(word, end=' ')\n",
    "    word = cprob_pride_2gram[word].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we can successfully generate a random text from a file pulled from a url, this will be useful in future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this intermediate report we were able to: \n",
    "- Generate random text from a single file using 2 techniques\n",
    "- Retrieve works from the nltk Gutenberg corpus\n",
    "- Retrieve a text file from a URL and generate a text from it\n",
    "\n",
    "The next steps will be: \n",
    "- Retrieve multiple texts from URLs\n",
    "- Combine multiple books for each author\n",
    "- Generate a text based on two texts/authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
